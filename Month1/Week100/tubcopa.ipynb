{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10189411,"sourceType":"datasetVersion","datasetId":6295277}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport gc\nfrom tqdm import tqdm\n\n# Set device and print debug info\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")\n    print(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n\nclass ChestXrayDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.classes = ['COVID-19', 'Normal', 'Pneumonia', 'Tuberculosis']\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n        self.images = []\n        self.labels = []\n        \n        # Load all image paths and labels\n        for class_name in self.classes:\n            class_path = os.path.join(data_dir, class_name)\n            if not os.path.exists(class_path):\n                raise ValueError(f\"Path does not exist: {class_path}\")\n            files = os.listdir(class_path)\n            print(f\"Found {len(files)} images in {class_name} class\")\n            for img_name in files:\n                self.images.append(os.path.join(class_path, img_name))\n                self.labels.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        try:\n            image = Image.open(img_path).convert('RGB')\n            label = self.labels[idx]\n\n            if self.transform:\n                image = self.transform(image)\n\n            return image, torch.tensor(label, dtype=torch.long)\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {str(e)}\")\n            raise e\n\ndef create_data_loaders(base_dir, batch_size=32):\n    print(f\"Checking directory: {base_dir}\")\n    print(f\"Directory exists: {os.path.exists(base_dir)}\")\n\n    # Enhanced data augmentation for training\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.2),\n        transforms.RandomRotation(30),\n        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n        transforms.RandomAutocontrast(p=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    # Validation/Testing transform\n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    # Create datasets\n    full_dataset = ChestXrayDataset(base_dir, transform=train_transform)\n    print(f\"Total dataset size: {len(full_dataset)}\")\n    \n    # Split dataset\n    train_size = int(0.7 * len(full_dataset))\n    val_size = int(0.15 * len(full_dataset))\n    test_size = len(full_dataset) - train_size - val_size\n    \n    print(f\"Train size: {train_size}, Val size: {val_size}, Test size: {test_size}\")\n    \n    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size, test_size]\n    )\n\n    # Update transforms for validation and test datasets\n    val_dataset.dataset.transform = val_transform\n    test_dataset.dataset.transform = val_transform\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n    return train_loader, val_loader, test_loader\n\nclass ChestXrayModel(nn.Module):\n    def __init__(self, num_classes=4):\n        super(ChestXrayModel, self).__init__()\n        print(\"Initializing model...\")\n        \n        # Load pre-trained DenseNet169 (upgraded from 121)\n        self.densenet = models.densenet169(weights='IMAGENET1K_V1')\n        \n        # Freeze early layers\n        for param in list(self.densenet.parameters())[:-60]:\n            param.requires_grad = False\n            \n        # Modified classifier for 4 classes\n        num_features = self.densenet.classifier.in_features\n        self.densenet.classifier = nn.Sequential(\n            nn.BatchNorm1d(num_features),\n            nn.Linear(num_features, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n        print(\"Model initialization completed\")\n\n    def forward(self, x):\n        return self.densenet(x)\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    pbar = tqdm(train_loader, desc='Training')\n    for batch_idx, (inputs, labels) in enumerate(pbar):\n        try:\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            pbar.set_postfix({'loss': running_loss/(batch_idx+1), 'acc': 100.*correct/total})\n            \n        except Exception as e:\n            print(f\"Error in batch {batch_idx}: {str(e)}\")\n            raise e\n    \n    return running_loss/len(train_loader), correct/total\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    return running_loss/len(val_loader), correct/total\n\ndef plot_confusion_matrix(model, test_loader, device):\n    model.eval()\n    all_predictions = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    # Calculate and plot confusion matrix\n    cm = confusion_matrix(all_labels, all_predictions)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['COVID-19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS'],\n                yticklabels=['COVID-19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS'])\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_labels, all_predictions, \n                              target_names=['COVID-19', 'NORMAL', 'PNEUMONIA', 'TUBERCULOSIS']))\n\ndef main():\n    # Hyperparameters\n    BATCH_SIZE = 32\n    EPOCHS = 20\n    LEARNING_RATE = 1e-4\n    BASE_DIR = \"/kaggle/input/3-diseases-dataset/Dataset\"  # Update this path\n    \n    try:\n        print(\"\\n1. Creating data loaders...\")\n        train_loader, val_loader, test_loader = create_data_loaders(\n            BASE_DIR,\n            batch_size=BATCH_SIZE\n        )\n\n        print(\"\\n2. Creating model...\")\n        model = ChestXrayModel(num_classes=4).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.2)\n\n        print(\"\\n3. Training model...\")\n        train_metrics = {'loss': [], 'acc': []}\n        val_metrics = {'loss': [], 'acc': []}\n        best_val_loss = float('inf')\n        \n        for epoch in range(EPOCHS):\n            print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n            \n            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n            val_loss, val_acc = validate(model, val_loader, criterion, device)\n            \n            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n            \n            train_metrics['loss'].append(train_loss)\n            train_metrics['acc'].append(train_acc)\n            val_metrics['loss'].append(val_loss)\n            val_metrics['acc'].append(val_acc)\n            \n            scheduler.step(val_loss)\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), 'best_model.pth')\n\n        print(\"\\n4. Plotting metrics...\")\n        plot_metrics(train_metrics, val_metrics)\n\n        print(\"\\n5. Generating confusion matrix...\")\n        plot_confusion_matrix(model, test_loader, device)\n\n        print(\"\\n6. Saving final model...\")\n        torch.save(model.state_dict(), 'final_model.pth')\n\n    except Exception as e:\n        print(f\"\\nError occurred: {str(e)}\")\n        raise e\n\n    finally:\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T11:29:47.355806Z","iopub.execute_input":"2024-12-13T11:29:47.356139Z","iopub.status.idle":"2024-12-13T11:29:47.673066Z","shell.execute_reply.started":"2024-12-13T11:29:47.356113Z","shell.execute_reply":"2024-12-13T11:29:47.671941Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nInitial GPU Memory: 0.00 MB\n\n1. Creating data loaders...\nChecking directory: /kaggle/input/3-diseases-dataset/Dataset\nDirectory exists: True\nFound 4450 images in COVID-19 class\n\nError occurred: Path does not exist: /kaggle/input/3-diseases-dataset/Dataset/Normal\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 283\u001b[0m\n\u001b[1;32m    280\u001b[0m             torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 275\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n","Cell \u001b[0;32mIn[2], line 228\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. Creating data loaders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m     train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Creating model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m     model \u001b[38;5;241m=\u001b[39m ChestXrayModel(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[2], line 85\u001b[0m, in \u001b[0;36mcreate_data_loaders\u001b[0;34m(base_dir, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     79\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     80\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     81\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     82\u001b[0m ])\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXrayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 36\u001b[0m, in \u001b[0;36mChestXrayDataset.__init__\u001b[0;34m(self, data_dir, transform)\u001b[0m\n\u001b[1;32m     34\u001b[0m class_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, class_name)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(class_path):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(class_path)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Path does not exist: /kaggle/input/3-diseases-dataset/Dataset/Normal"],"ename":"ValueError","evalue":"Path does not exist: /kaggle/input/3-diseases-dataset/Dataset/Normal","output_type":"error"}],"execution_count":2}]}